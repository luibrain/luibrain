---
title: "laba1"
author: "Semin A.S."
date: '20 февраля 2018 г '
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Лабораторная работа №1

Перед нами стоит задача выяснить как меняется поведение ошибок на тестовой и обучающей выборках при различном числе степеней свободы, если функция зависимости отклика Y от единственного признака X известна.

X∼U(5,105)

Y=f(X)+ϵ, где f(X)=15 - 0.05*X; ϵ∼N(0,1).

Изобразим 2 различных случая. Второй случай будет аналогичен первому, за исключением значения параметра train.percent=0.5.

# 1 случай

```{r plots, echo = FALSE}
# ядро 
my.seed <- 1486372882 

# наблюдений всего 
n.all <- 60 
# доля обучающей выборки 
train.percent <- 0.85 
# стандартное отклонение случайного шума 
res.sd <- 1 
# границы изменения X 
x.min <- 5 
x.max <- 105 

# фактические значения x 
set.seed(my.seed) 
x <- runif(x.min, x.max, n = n.all) 

# случайный шум 
set.seed(my.seed) 
res <- rnorm(mean = 0, sd = res.sd, n = n.all) 
# отбираем наблюдения в обучающую выборку 
set.seed(my.seed) 
inTrain <- sample(seq_along(x), size = train.percent*n.all) 

# истинная функция взаимосвязи 
y.func <- function(x) {12 - 0.05*x} 

# для графика истинной взаимосвязи 
x.line <- seq(x.min, x.max, length = n.all) 
y.line <- y.func(x.line) 

# фактические значения y (с шумом) 
y <- y.func(x) + res 

# Создаём векторы с данными для построения графиков ############################ 

# наблюдения на обучающей выборке 
x.train <- x[inTrain] 
y.train <- y[inTrain] 

# наблюдения на тестовой выборке 
x.test <- x[-inTrain] 
y.test <- y[-inTrain] 
# График 1: Исходные данные на график ######################################### 

# убираем широкие поля рисунка 
par(mar = c(4, 4, 1, 1)) 

# наименьшие/наибольшие значения по осям 
x.lim <- c(x.min, x.max) 
y.lim <- c(min(y), max(y)) 

# наблюдения с шумом (обучающая выборка) 
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21, 
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)


# заголовок 
mtext('Исходные данные и истинная функция связи', side = 3) 

# наблюдения тестовой выборки 
points(x.test, y.test, col = 'red', bg = 'red', pch = 21) 

# истинная функция 
lines(x.line, y.line, lwd = 2, lty = 2) 

# легенда 
legend('topleft', legend = c('обучение', 'тест', 'f(X)'), 
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'), 
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

# Строим модель ############################################################### 
# Цикл по степеням свободы #################################################### 

# максимальное число степеней свободы для модели сплайна 
max.df <- 40 
# таблица для записи ошибок 
tbl <- data.frame(df = 2:max.df) 
# ошибки на обучающей выборке 
tbl$MSE.train <- 0 
# ошибки на тестовой выборке 
tbl$MSE.test <- 0
for (i in 2:max.df) { 
  # строим модель 
  mod <- smooth.spline(x = x.train, y = y.train, df = i) 
  
  # модельные значения для расчёта ошибок 
  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1] 
  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1] 
  
  # считаем средний квадрат ошибки на обечающей и тестовой выборке 
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train), 
           sum((y.test - y.model.test)^2) / length(x.test)) 
  
  # записываем ошибки в модель 
  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE 
} 

# График 2: Зависимость MSE от гибкости модели ################################ 
plot(x = tbl$df, y = tbl$MSE.test, 
     type = 'l', col = 'red', lwd = 2, 
     xlab = 'Степени свободы сплайна', ylab = 'MSE', 
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)), 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) 

# заголовок 
mtext('Изменение MSE с ростом числа степеней свободы', side = 3) 

points(x = tbl$df, y = tbl$MSE.test, 
       pch = 21, col = 'red', bg = 'red') 
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) 
# неустранимая ошибка 
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2) 

# легенда 
legend('topleft', legend = c('обучающая', 'тестовая'), 
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'), 
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# степени свободы у наименьшей ошибки на тестовой выборке 
min.MSE.test <- min(tbl$MSE.test) 
df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df'] 

# компромисс между точностью и простотой модели по графику 
df.my.MSE.test <- df.min.MSE.test 
my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']
# ставим точку на графике 
abline(v = df.my.MSE.test, 
       lty = 2, lwd = 2) 
points(x = df.my.MSE.test, y = my.MSE.test, 
       pch = 15, col = 'blue') 
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2) 

# График 3: Лучшая модель (компромисс между гибкостью и точностью) ############ 

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) 

# для гладких графиков модели 
x.model.plot <- seq(x.min, x.max, length = 250) 
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] 

# убираем широкие поля рисунка 
par(mar = c(4, 4, 1, 1)) 

# наименьшие/наибольшие значения по осям 
x.lim <- c(x.min, x.max) 
y.lim <- c(min(y), max(y)) 

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21, 
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) 

# заголовок 
mtext('Исходные данные и лучшая модель', side = 3) 

# наблюдения тестовой выборки 
points(x.test, y.test, 
       col = 'red', bg = 'red', pch = 21) 

# истинная функция 
lines(x.line, y.line, 
      lwd = 2, lty = 2) 

# модель 
lines(x.model.plot, y.model.plot, 
      lwd = 2, col = 'blue') 

# легенда 
legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'), 
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'), 
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
```

# 2 случай

```{r plots1, echo = FALSE, warning=FALSE}
# Генерируем данные ########################################################### 

# ядро 
my.seed <- 1486372882 

# наблюдений всего 
n.all <- 60 
# доля обучающей выборки 
train.percent <- 0.5 
# стандартное отклонение случайного шума 
res.sd <- 1 
# границы изменения X 
x.min <- 5 
x.max <- 105 

# фактические значения x 
set.seed(my.seed) 
x <- runif(x.min, x.max, n = n.all) 

# случайный шум 
set.seed(my.seed) 
res <- rnorm(mean = 0, sd = res.sd, n = n.all) 
# отбираем наблюдения в обучающую выборку 
set.seed(my.seed) 
inTrain <- sample(seq_along(x), size = train.percent*n.all) 

# истинная функция взаимосвязи 
y.func <- function(x) {12 - 0.05*x} 

# для графика истинной взаимосвязи 
x.line <- seq(x.min, x.max, length = n.all) 
y.line <- y.func(x.line) 

# фактические значения y (с шумом) 
y <- y.func(x) + res 

# Создаём векторы с данными для построения графиков ############################ 

# наблюдения на обучающей выборке 
x.train <- x[inTrain] 
y.train <- y[inTrain] 

# наблюдения на тестовой выборке 
x.test <- x[-inTrain] 
y.test <- y[-inTrain] 

# График 1: Исходные данные на график ######################################### 

# убираем широкие поля рисунка 
par(mar = c(4, 4, 1, 1)) 

# наименьшие/наибольшие значения по осям 
x.lim <- c(x.min, x.max) 
y.lim <- c(min(y), max(y)) 

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21, 
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) 

# заголовок 
mtext('Исходные данные и истинная функция связи', side = 3) 

# наблюдения тестовой выборки 
points(x.test, y.test, col = 'red', bg = 'red', pch = 21) 

# истинная функция 
lines(x.line, y.line, lwd = 2, lty = 2) 

# легенда 
legend('topleft', legend = c('обучение', 'тест', 'f(X)'), 
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'), 
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

# Строим модель ############################################################### 
# Цикл по степеням свободы #################################################### 

# максимальное число степеней свободы для модели сплайна 
max.df <- 40 
# таблица для записи ошибок 
tbl <- data.frame(df = 2:max.df) 
# ошибки на обучающей выборке 
tbl$MSE.train <- 0 
# ошибки на тестовой выборке 
tbl$MSE.test <- 0

for (i in 2:max.df) { 
  # строим модель 
  mod <- smooth.spline(x = x.train, y = y.train, df = i) 
  
  # модельные значения для расчёта ошибок 
  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1] 
  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1] 
  
  # считаем средний квадрат ошибки на обечающей и тестовой выборке 
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train), 
           sum((y.test - y.model.test)^2) / length(x.test)) 
  
  # записываем ошибки в модель 
  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE 
} 

# График 2: Зависимость MSE от гибкости модели ################################ 
plot(x = tbl$df, y = tbl$MSE.test, 
     type = 'l', col = 'red', lwd = 2, 
     xlab = 'Степени свободы сплайна', ylab = 'MSE', 
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)), 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) 

# заголовок 
mtext('Изменение MSE с ростом числа степеней свободы', side = 3) 

points(x = tbl$df, y = tbl$MSE.test, 
       pch = 21, col = 'red', bg = 'red') 
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) 
# неустранимая ошибка 
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2) 

# легенда 
legend('topleft', legend = c('обучающая', 'тестовая'), 
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'), 
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# степени свободы у наименьшей ошибки на тестовой выборке 
min.MSE.test <- min(tbl$MSE.test) 
df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df'] 

# компромисс между точностью и простотой модели по графику 
df.my.MSE.test <- df.min.MSE.test 
my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']
# ставим точку на графике 
abline(v = df.my.MSE.test, 
       lty = 2, lwd = 2) 
points(x = df.my.MSE.test, y = my.MSE.test, 
       pch = 15, col = 'blue') 
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2) 

# График 3: Лучшая модель (компромисс между гибкостью и точностью) ############ 

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) 

# для гладких графиков модели 
x.model.plot <- seq(x.min, x.max, length = 250) 
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] 

# убираем широкие поля рисунка 
par(mar = c(4, 4, 1, 1)) 

# наименьшие/наибольшие значения по осям 
x.lim <- c(x.min, x.max) 
y.lim <- c(min(y), max(y)) 

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21, 
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) 

# заголовок 
mtext('Исходные данные и лучшая модель', side = 3) 

# наблюдения тестовой выборки 
points(x.test, y.test, 
       col = 'red', bg = 'red', pch = 21) 

# истинная функция 
lines(x.line, y.line, 
      lwd = 2, lty = 2) 

# модель 
lines(x.model.plot, y.model.plot, 
      lwd = 2, col = 'blue') 

# легенда 
legend('topleft', legend = c('обучение', 'тест', 'f(X)', 'модель'), 
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'), 
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
```

При движении слева направо в обоих случаях MSE на обучающей выборке (серая кривая) сокращается, потому что с ростом числа степеней свободы расчёт число узлов, по которым строится сплайн. При этом модельная кривая подгоняется по всё возрастающему количеству точек и становится всё более гибкой. В результате индивидуальные расстояния от фактических наблюдений за Y до их модельных оценок сокращаются, что приводит к сокращению MSE.

При движении слева направо в обоих случаях MSE на тестовой выборке (красная кривая) растет, потому что дана обычная линейная функция вида Y = kX + b. В случае, где процент тестовой выборки составляет 50%, в конце графика, наблюдается стабильность, т.е. модель все лучше и лучше описывает обучающую выборку.